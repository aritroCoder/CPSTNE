{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kxe3HtcAEkoo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_file = (\n",
        "    \"mnist_data/mnist.pkl\"\n",
        ")\n",
        "\n",
        "with open(data_file, \"rb\") as f:\n",
        "    data = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.71428571, 0.96428571],\n",
              "       [0.53571429, 0.64285714],\n",
              "       [0.35714286, 0.32142857],\n",
              "       [0.57142857, 0.14285714],\n",
              "       [0.03571429, 0.53571429],\n",
              "       [0.92857143, 0.32142857],\n",
              "       [0.82142857, 0.03571429],\n",
              "       [0.89285714, 0.75      ],\n",
              "       [0.10714286, 0.32142857],\n",
              "       [0.75      , 0.10714286],\n",
              "       [0.10714286, 0.85714286],\n",
              "       [0.46428571, 0.07142857],\n",
              "       [0.85714286, 0.64285714],\n",
              "       [0.28571429, 0.82142857],\n",
              "       [0.71428571, 0.42857143],\n",
              "       [0.28571429, 0.03571429],\n",
              "       [0.03571429, 0.17857143],\n",
              "       [0.5       , 0.75      ],\n",
              "       [0.14285714, 0.5       ],\n",
              "       [0.71428571, 0.92857143],\n",
              "       [0.28571429, 0.53571429],\n",
              "       [0.07142857, 0.32142857],\n",
              "       [0.71428571, 0.67857143],\n",
              "       [0.46428571, 0.71428571],\n",
              "       [0.67857143, 0.14285714],\n",
              "       [0.78571429, 0.89285714],\n",
              "       [0.42857143, 0.78571429],\n",
              "       [0.21428571, 0.32142857],\n",
              "       [0.03571429, 0.46428571],\n",
              "       [0.25      , 0.96428571],\n",
              "       [0.5       , 0.07142857],\n",
              "       [0.25      , 0.39285714],\n",
              "       [0.57142857, 0.46428571],\n",
              "       [0.57142857, 0.92857143],\n",
              "       [0.57142857, 0.85714286],\n",
              "       [0.42857143, 0.        ],\n",
              "       [0.32142857, 0.46428571],\n",
              "       [0.85714286, 0.17857143],\n",
              "       [0.39285714, 0.42857143],\n",
              "       [0.5       , 0.28571429],\n",
              "       [0.        , 0.5       ],\n",
              "       [0.21428571, 0.96428571],\n",
              "       [0.35714286, 0.53571429],\n",
              "       [0.85714286, 0.07142857],\n",
              "       [0.71428571, 0.57142857],\n",
              "       [0.07142857, 0.5       ],\n",
              "       [0.60714286, 0.53571429],\n",
              "       [0.03571429, 0.42857143],\n",
              "       [0.17857143, 0.60714286],\n",
              "       [0.        , 0.10714286],\n",
              "       [0.57142857, 0.82142857],\n",
              "       [0.71428571, 0.07142857],\n",
              "       [0.78571429, 0.10714286],\n",
              "       [0.82142857, 0.89285714],\n",
              "       [0.82142857, 0.17857143],\n",
              "       [0.14285714, 0.85714286],\n",
              "       [0.14285714, 0.53571429],\n",
              "       [0.28571429, 0.10714286],\n",
              "       [0.82142857, 0.14285714],\n",
              "       [0.39285714, 0.        ],\n",
              "       [0.89285714, 0.67857143],\n",
              "       [0.39285714, 0.46428571],\n",
              "       [0.64285714, 0.25      ],\n",
              "       [0.14285714, 0.14285714],\n",
              "       [0.53571429, 0.85714286],\n",
              "       [0.14285714, 0.96428571],\n",
              "       [0.35714286, 0.67857143],\n",
              "       [0.10714286, 0.39285714],\n",
              "       [0.67857143, 0.28571429],\n",
              "       [0.5       , 0.03571429],\n",
              "       [0.92857143, 0.28571429],\n",
              "       [0.82142857, 0.21428571],\n",
              "       [0.21428571, 0.67857143],\n",
              "       [0.53571429, 0.5       ],\n",
              "       [0.28571429, 0.17857143],\n",
              "       [0.96428571, 0.25      ],\n",
              "       [0.25      , 0.92857143],\n",
              "       [0.92857143, 0.60714286],\n",
              "       [0.14285714, 0.57142857],\n",
              "       [0.53571429, 0.71428571],\n",
              "       [0.85714286, 0.32142857],\n",
              "       [0.53571429, 0.78571429],\n",
              "       [0.53571429, 0.96428571],\n",
              "       [0.46428571, 0.14285714],\n",
              "       [0.71428571, 0.        ],\n",
              "       [0.        , 0.85714286],\n",
              "       [0.42857143, 0.67857143],\n",
              "       [0.21428571, 0.75      ],\n",
              "       [0.92857143, 0.25      ],\n",
              "       [0.57142857, 0.03571429],\n",
              "       [0.85714286, 0.82142857],\n",
              "       [0.64285714, 0.5       ],\n",
              "       [0.42857143, 0.85714286],\n",
              "       [0.78571429, 0.21428571],\n",
              "       [0.85714286, 0.35714286],\n",
              "       [0.21428571, 0.64285714],\n",
              "       [0.25      , 0.78571429],\n",
              "       [0.03571429, 0.21428571],\n",
              "       [0.5       , 0.17857143],\n",
              "       [0.14285714, 0.64285714],\n",
              "       [0.07142857, 0.46428571],\n",
              "       [0.75      , 0.57142857],\n",
              "       [0.82142857, 0.78571429],\n",
              "       [0.32142857, 0.53571429],\n",
              "       [0.        , 0.92857143],\n",
              "       [0.64285714, 0.82142857],\n",
              "       [0.42857143, 0.25      ],\n",
              "       [0.32142857, 0.03571429],\n",
              "       [0.78571429, 0.78571429],\n",
              "       [0.42857143, 0.53571429],\n",
              "       [0.46428571, 0.28571429],\n",
              "       [0.28571429, 0.5       ],\n",
              "       [0.78571429, 0.67857143],\n",
              "       [0.67857143, 0.21428571],\n",
              "       [0.96428571, 0.        ],\n",
              "       [0.28571429, 0.46428571],\n",
              "       [0.17857143, 0.46428571],\n",
              "       [0.78571429, 0.25      ],\n",
              "       [0.17857143, 0.96428571],\n",
              "       [0.60714286, 0.07142857],\n",
              "       [0.96428571, 0.32142857],\n",
              "       [0.32142857, 0.75      ],\n",
              "       [0.21428571, 0.46428571],\n",
              "       [0.67857143, 0.5       ],\n",
              "       [0.5       , 0.39285714],\n",
              "       [0.42857143, 0.92857143],\n",
              "       [0.92857143, 0.5       ],\n",
              "       [0.85714286, 0.21428571],\n",
              "       [0.85714286, 0.39285714],\n",
              "       [0.21428571, 0.71428571],\n",
              "       [0.92857143, 0.53571429],\n",
              "       [0.28571429, 0.85714286],\n",
              "       [0.25      , 0.28571429],\n",
              "       [0.03571429, 0.32142857],\n",
              "       [0.10714286, 0.07142857],\n",
              "       [0.32142857, 0.21428571],\n",
              "       [0.85714286, 0.25      ],\n",
              "       [0.96428571, 0.92857143],\n",
              "       [0.75      , 0.89285714],\n",
              "       [0.64285714, 0.07142857],\n",
              "       [0.85714286, 0.53571429],\n",
              "       [0.64285714, 0.57142857],\n",
              "       [0.78571429, 0.35714286],\n",
              "       [0.78571429, 0.32142857],\n",
              "       [0.96428571, 0.42857143],\n",
              "       [0.35714286, 0.5       ],\n",
              "       [0.89285714, 0.82142857],\n",
              "       [0.        , 0.75      ],\n",
              "       [0.14285714, 0.28571429],\n",
              "       [0.46428571, 0.42857143],\n",
              "       [0.21428571, 0.14285714],\n",
              "       [0.89285714, 0.17857143],\n",
              "       [0.28571429, 0.42857143],\n",
              "       [0.        , 0.57142857],\n",
              "       [0.96428571, 0.60714286],\n",
              "       [0.82142857, 0.        ],\n",
              "       [0.89285714, 0.07142857],\n",
              "       [0.60714286, 0.64285714],\n",
              "       [0.39285714, 0.39285714],\n",
              "       [0.32142857, 0.64285714],\n",
              "       [0.89285714, 0.60714286],\n",
              "       [0.39285714, 0.07142857],\n",
              "       [0.5       , 0.5       ],\n",
              "       [0.5       , 0.89285714],\n",
              "       [0.64285714, 0.46428571],\n",
              "       [0.14285714, 0.17857143],\n",
              "       [0.78571429, 0.39285714],\n",
              "       [0.82142857, 0.32142857],\n",
              "       [0.07142857, 0.89285714],\n",
              "       [0.10714286, 0.17857143],\n",
              "       [0.46428571, 0.85714286],\n",
              "       [0.25      , 0.        ],\n",
              "       [0.64285714, 0.92857143],\n",
              "       [0.92857143, 0.89285714],\n",
              "       [0.75      , 0.71428571],\n",
              "       [0.39285714, 0.17857143],\n",
              "       [0.03571429, 0.5       ],\n",
              "       [0.46428571, 0.89285714],\n",
              "       [0.46428571, 0.03571429],\n",
              "       [0.35714286, 0.60714286],\n",
              "       [0.67857143, 0.07142857],\n",
              "       [0.71428571, 0.60714286],\n",
              "       [0.21428571, 0.35714286],\n",
              "       [0.10714286, 0.35714286],\n",
              "       [0.39285714, 0.35714286],\n",
              "       [0.96428571, 0.28571429],\n",
              "       [0.82142857, 0.64285714],\n",
              "       [0.5       , 0.53571429],\n",
              "       [0.46428571, 0.21428571],\n",
              "       [0.07142857, 0.14285714],\n",
              "       [0.35714286, 0.28571429],\n",
              "       [0.64285714, 0.53571429],\n",
              "       [0.03571429, 0.85714286],\n",
              "       [0.82142857, 0.25      ],\n",
              "       [0.39285714, 0.28571429],\n",
              "       [0.78571429, 0.28571429],\n",
              "       [0.39285714, 0.5       ],\n",
              "       [0.5       , 0.82142857],\n",
              "       [0.53571429, 0.82142857],\n",
              "       [0.17857143, 0.53571429],\n",
              "       [0.67857143, 0.67857143],\n",
              "       [0.42857143, 0.71428571],\n",
              "       [0.25      , 0.25      ],\n",
              "       [0.07142857, 0.82142857],\n",
              "       [0.28571429, 0.39285714],\n",
              "       [0.32142857, 0.25      ],\n",
              "       [0.71428571, 0.5       ],\n",
              "       [0.96428571, 0.21428571],\n",
              "       [0.57142857, 0.32142857],\n",
              "       [0.64285714, 0.        ],\n",
              "       [0.46428571, 0.39285714],\n",
              "       [0.96428571, 0.85714286],\n",
              "       [0.42857143, 0.10714286],\n",
              "       [0.21428571, 0.60714286],\n",
              "       [0.57142857, 0.67857143],\n",
              "       [0.28571429, 0.67857143],\n",
              "       [0.17857143, 0.89285714],\n",
              "       [0.78571429, 0.57142857],\n",
              "       [0.96428571, 0.82142857],\n",
              "       [0.67857143, 0.64285714],\n",
              "       [0.32142857, 0.60714286],\n",
              "       [0.57142857, 0.57142857],\n",
              "       [0.67857143, 0.10714286],\n",
              "       [0.82142857, 0.28571429],\n",
              "       [0.57142857, 0.35714286],\n",
              "       [0.        , 0.03571429],\n",
              "       [0.67857143, 0.57142857],\n",
              "       [0.21428571, 0.78571429],\n",
              "       [0.        , 0.82142857],\n",
              "       [0.14285714, 0.07142857],\n",
              "       [0.        , 0.21428571],\n",
              "       [0.07142857, 0.17857143],\n",
              "       [0.10714286, 0.5       ],\n",
              "       [0.03571429, 0.03571429],\n",
              "       [0.42857143, 0.03571429],\n",
              "       [0.10714286, 0.42857143],\n",
              "       [0.14285714, 0.71428571],\n",
              "       [0.57142857, 0.5       ],\n",
              "       [0.03571429, 0.60714286],\n",
              "       [0.35714286, 0.39285714],\n",
              "       [0.57142857, 0.25      ],\n",
              "       [0.14285714, 0.32142857],\n",
              "       [0.32142857, 0.32142857],\n",
              "       [0.92857143, 0.71428571],\n",
              "       [0.82142857, 0.92857143],\n",
              "       [0.75      , 0.03571429],\n",
              "       [0.42857143, 0.32142857],\n",
              "       [0.10714286, 0.67857143],\n",
              "       [0.92857143, 0.10714286],\n",
              "       [0.71428571, 0.03571429],\n",
              "       [0.32142857, 0.42857143],\n",
              "       [0.85714286, 0.42857143],\n",
              "       [0.5       , 0.64285714],\n",
              "       [0.17857143, 0.35714286],\n",
              "       [0.03571429, 0.82142857],\n",
              "       [0.        , 0.60714286],\n",
              "       [0.5       , 0.57142857],\n",
              "       [0.39285714, 0.67857143],\n",
              "       [0.46428571, 0.5       ],\n",
              "       [0.39285714, 0.10714286],\n",
              "       [0.57142857, 0.71428571],\n",
              "       [0.5       , 0.        ],\n",
              "       [0.64285714, 0.96428571],\n",
              "       [0.89285714, 0.10714286],\n",
              "       [0.5       , 0.14285714],\n",
              "       [0.57142857, 0.07142857],\n",
              "       [0.71428571, 0.78571429],\n",
              "       [0.03571429, 0.96428571],\n",
              "       [0.21428571, 0.28571429],\n",
              "       [0.78571429, 0.64285714],\n",
              "       [0.57142857, 0.78571429],\n",
              "       [0.89285714, 0.85714286],\n",
              "       [0.        , 0.        ],\n",
              "       [0.85714286, 0.92857143],\n",
              "       [0.42857143, 0.46428571],\n",
              "       [0.03571429, 0.28571429],\n",
              "       [0.46428571, 0.10714286],\n",
              "       [0.03571429, 0.        ],\n",
              "       [0.21428571, 0.        ],\n",
              "       [0.46428571, 0.75      ],\n",
              "       [0.46428571, 0.25      ],\n",
              "       [0.82142857, 0.96428571],\n",
              "       [0.14285714, 0.75      ],\n",
              "       [0.89285714, 0.21428571],\n",
              "       [0.60714286, 0.46428571],\n",
              "       [0.39285714, 0.75      ],\n",
              "       [0.42857143, 0.64285714],\n",
              "       [0.64285714, 0.14285714],\n",
              "       [0.07142857, 0.28571429],\n",
              "       [0.71428571, 0.53571429],\n",
              "       [0.14285714, 0.03571429],\n",
              "       [0.17857143, 0.28571429],\n",
              "       [0.07142857, 0.03571429],\n",
              "       [0.35714286, 0.75      ],\n",
              "       [0.28571429, 0.21428571],\n",
              "       [0.85714286, 0.03571429],\n",
              "       [0.96428571, 0.64285714],\n",
              "       [0.64285714, 0.21428571],\n",
              "       [0.64285714, 0.75      ],\n",
              "       [0.        , 0.14285714]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['meta_train_x'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "D3mh1OfBWjAe"
      },
      "outputs": [],
      "source": [
        "class FOMDataset(Dataset):\n",
        "    def __init__(self, csv_file):\n",
        "        # Load the CSV file\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        # Extract features (thickness, wavelength) and target (fom)\n",
        "        self.X = self.data[['thickness', 'wavelength']].values\n",
        "        self.y = self.data['fom'].values\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of samples\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the input features and target for a given index\n",
        "        x = torch.tensor(self.X[idx], dtype=torch.float32)\n",
        "        y = torch.tensor(self.y[idx], dtype=torch.float32).unsqueeze(0)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SinusoidGenerator(Dataset):\n",
        "    def __init__(self, train=True, few_k_shot=20):\n",
        "        self.few_k_shot = few_k_shot\n",
        "        data_file = (\n",
        "            \"sinusoidal_data/sinusoid_data/sinusoidal_train.pkl\"\n",
        "            if train\n",
        "            else \"sinusoidal_data/sinusoid_data/sinusoidal_test.pkl\"\n",
        "        )\n",
        "\n",
        "        with open(data_file, \"rb\") as f:\n",
        "            data = pickle.load(f)\n",
        "\n",
        "        self.data = {\n",
        "            \"train_x\": torch.tensor(data[\"x\"][:, : self.few_k_shot, :]),\n",
        "            \"train_y\": torch.tensor(data[\"y\"][:, : self.few_k_shot, :]),\n",
        "            \"test_x\": torch.tensor(data[\"x\"][:, 20:, :]),\n",
        "            \"test_y\": torch.tensor(data[\"y\"][:, 20:, :]),\n",
        "        }\n",
        "\n",
        "        print(\n",
        "            \"load data: train_x\",\n",
        "            self.data[\"train_x\"].shape,\n",
        "            \"val_x\",\n",
        "            self.data[\"test_x\"].shape,\n",
        "            \"train_y\",\n",
        "            self.data[\"train_y\"].shape,\n",
        "            \"val_y\",\n",
        "            self.data[\"test_y\"].shape,\n",
        "        )\n",
        "\n",
        "        self.train = train\n",
        "        self.dim_input = 1\n",
        "        self.dim_output = 1\n",
        "\n",
        "    def generate_batch(self, indx):\n",
        "        context_x = self.data[\"train_x\"][indx]\n",
        "        context_y = self.data[\"train_y\"][indx]\n",
        "        target_x = self.data[\"test_x\"][indx]\n",
        "        target_y = self.data[\"test_y\"][indx]\n",
        "\n",
        "        if self.train:\n",
        "            return context_x, context_y, target_x, target_y\n",
        "        else:\n",
        "            return torch.cat((context_x, target_x)), torch.cat((context_y, target_y))\n",
        "    \n",
        "    def __len__(self):\n",
        "        if self.train:\n",
        "            return self.data[\"train_x\"].shape[0]\n",
        "        else:\n",
        "            return self.data[\"test_x\"].shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.generate_batch(idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "M_vcSQA5exMi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load data: train_x torch.Size([240000, 20, 1]) val_x torch.Size([240000, 10, 1]) train_y torch.Size([240000, 20, 1]) val_y torch.Size([240000, 10, 1])\n",
            "load data: train_x torch.Size([100, 20, 1]) val_x torch.Size([100, 100, 1]) train_y torch.Size([100, 20, 1]) val_y torch.Size([100, 100, 1])\n"
          ]
        }
      ],
      "source": [
        "sine_dataset_train = SinusoidGenerator(train=True, few_k_shot=20)\n",
        "sine_dataset_test = SinusoidGenerator(train=False)\n",
        "\n",
        "sine_dataloader_train = DataLoader(\n",
        "    sine_dataset_train, batch_size=1, shuffle=True, num_workers=4\n",
        ")\n",
        "sine_dataloader_test = DataLoader(\n",
        "    sine_dataset_test, batch_size=1, shuffle=False, num_workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FDhYtDDfBah",
        "outputId": "7ee9e5a7-d339-4521-9e3f-9cf968dbc2c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 20, 1]) torch.Size([1, 20, 1]) torch.Size([1, 10, 1]) torch.Size([1, 10, 1])\n",
            "torch.Size([1, 120, 1]) torch.Size([1, 120, 1])\n"
          ]
        }
      ],
      "source": [
        "for t in sine_dataloader_train:\n",
        "  print(t[0].shape, t[1].shape, t[2].shape, t[3].shape)\n",
        "  break\n",
        "\n",
        "for t in sine_dataloader_test:\n",
        "  print(t[0].shape, t[1].shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "KUoezUclWpye"
      },
      "outputs": [],
      "source": [
        "# Create the dataset and DataLoader\n",
        "# csv_file = '/content/drive/MyDrive/IITP/Capstone II/fom.csv'  # Replace with the actual path to your CSV file\n",
        "# fom_dataset = FOMDataset(csv_file)\n",
        "# fom_dataloader = DataLoader(fom_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1JWFStrj6vj"
      },
      "source": [
        "## Model definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "EjV93wIFWct6"
      },
      "outputs": [],
      "source": [
        "# baseline for testing (similiar in paper)\n",
        "class BasisFunctionLearner(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(BasisFunctionLearner, self).__init__()\n",
        "        # Define a fully connected network with ReLU activations\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc6 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.relu(self.fc4(x))\n",
        "        x = self.relu(self.fc5(x))\n",
        "        return self.fc6(x)  # This represents the basis functions, Φ(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "2dP9oDmnRbrg"
      },
      "outputs": [],
      "source": [
        "# as per paper\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        self.query = nn.Linear(input_dim, hidden_dim)\n",
        "        self.key = nn.Linear(input_dim, hidden_dim)\n",
        "        self.value = nn.Linear(input_dim, hidden_dim)\n",
        "        self.ff_layer_0 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
        "        self.ff_layer_1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        query = self.query(x)\n",
        "        key = self.key(x)\n",
        "        value = self.value(x)\n",
        "\n",
        "        dotp = torch.matmul(query, key.transpose(-2, -1)) / (query.size(-1) ** 0.5)\n",
        "        attention_weights = F.softmax(dotp, dim=-1)\n",
        "        weighted_sum = torch.matmul(attention_weights, value)\n",
        "        x = weighted_sum + query  # Adding the query for residual connection\n",
        "        x = F.layer_norm(x, x.size()[1:])\n",
        "\n",
        "        dense_out_0 = F.relu(self.ff_layer_0(x))\n",
        "        x = x + self.ff_layer_1(dense_out_0)\n",
        "        x = F.layer_norm(x, x.size()[1:])\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "DlpM28ioRmJ-"
      },
      "outputs": [],
      "source": [
        "# as per paper\n",
        "\n",
        "class WeightsGenerator(nn.Module):\n",
        "    def __init__(self, basis_function_dim, hidden_dim=512, attention_layers=8):\n",
        "        super(WeightsGenerator, self).__init__()\n",
        "        self.top_attention_layer = AttentionLayer(basis_function_dim, hidden_dim)\n",
        "        self.attention_layers = nn.ModuleList([AttentionLayer(hidden_dim, hidden_dim) for _ in range(attention_layers)])\n",
        "\n",
        "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.final_dense = nn.Linear(hidden_dim, basis_function_dim)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = inputs\n",
        "        x = self.top_attention_layer(x)\n",
        "        for attention_layer in self.attention_layers:\n",
        "            x = attention_layer(x)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        final_weights = self.final_dense(x)\n",
        "        return final_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "1cjz-3iGEuiT"
      },
      "outputs": [],
      "source": [
        "class FewShotRegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, basis_function_dim, output_dim):\n",
        "        super(FewShotRegressionModel, self).__init__()\n",
        "        self.basis_learner = BasisFunctionLearner(input_dim, hidden_dim, basis_function_dim)\n",
        "        self.weights_generator = WeightsGenerator(basis_function_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Step 1: Generate basis functions Φ(x)\n",
        "        basis_functions = self.basis_learner(x)\n",
        "\n",
        "        # Step 2: Generate weights w\n",
        "        weights = self.weights_generator(basis_functions)\n",
        "\n",
        "        # Step 3: Compute final prediction z = Φ(x) * w\n",
        "        return weights, torch.diag(torch.matmul(basis_functions, weights.T))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "JnwM9v6JEyhm"
      },
      "outputs": [],
      "source": [
        "def custom_loss_function(predictions, targets, weights, l1_lambda=0.001, l2_lambda=0.0001):\n",
        "    mse_loss = nn.MSELoss()(predictions, targets)\n",
        "    l1_loss = l1_lambda * torch.norm(weights, p=1)\n",
        "    l2_loss = l2_lambda * torch.norm(weights, p=2)\n",
        "    return mse_loss + l1_loss + l2_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I37D7bedj_P2"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of trainable parameters: 17989120\n"
          ]
        }
      ],
      "source": [
        "input_dim = 1  # For example, (x1, x2) as inputs would require 2 dim\n",
        "hidden_dim = 512\n",
        "basis_function_dim = 256\n",
        "output_dim = 1  # For example, z as output\n",
        "learning_rate = 0.005\n",
        "num_epochs = 60000\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = FewShotRegressionModel(\n",
        "    input_dim, hidden_dim, basis_function_dim, output_dim\n",
        ").to(device)\n",
        "\n",
        "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Number of trainable parameters: {num_trainable_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73a4vhLUE1bF",
        "outputId": "8a6a1d8e-3c17-4a00-f805-9669cc186a37"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 24167/240000 [1:01:27<9:08:53,  6.55it/s] \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[88], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 34\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Print loss every 10 epochs\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "input_dim = 1  # For example, (x1, x2) as inputs would require 2 dim\n",
        "hidden_dim = 512\n",
        "basis_function_dim = 256\n",
        "output_dim = 1  # For example, z as output\n",
        "learning_rate = 0.005\n",
        "num_epochs = 60000\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = FewShotRegressionModel(input_dim, hidden_dim, basis_function_dim, output_dim).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    for batch_x_train, batch_y_train, batch_x_val, batch_y_val in tqdm(\n",
        "        sine_dataloader_train\n",
        "    ):\n",
        "        # move to device\n",
        "        batch_x_train = batch_x_train.to(device, torch.float32).squeeze(0)\n",
        "        batch_y_train = batch_y_train.to(device, torch.float32).squeeze(0)\n",
        "        batch_x_val = batch_x_val.to(device, torch.float32).squeeze(0)\n",
        "        batch_y_val = batch_y_val.to(device, torch.float32).squeeze(0)\n",
        "        \n",
        "        batch_size = batch_x_train.shape[0]\n",
        "\n",
        "        loss = 0\n",
        "        wts, predictions = model(batch_x_train)\n",
        "\n",
        "        loss += custom_loss_function(predictions, batch_y_train, wts)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print loss every 10 epochs\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmT-17Z-E6Aj",
        "outputId": "f5d2fa83-1be2-433a-b0c3-83c1037baaf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted z for input (29.84, 1099.5): 1381.579345703125\n"
          ]
        }
      ],
      "source": [
        "# After training, you can use the model to make predictions on new data\n",
        "x_new = torch.tensor([[29.84, 1099.5]]).to(device)\n",
        "z_pred = model(x_new)\n",
        "print(f'Predicted z for input (29.84, 1099.5): {z_pred[1].item()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "a3vqVUQiZacL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "v1JWFStrj6vj"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
