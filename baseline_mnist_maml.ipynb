{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import learn2learn as l2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "batch_size = 128\n",
    "num_shot = 50  # Example values, can be adjusted\n",
    "num_val = 50\n",
    "input_dim = 2  # For example, (x1, x2) as inputs would require 2 dim\n",
    "hidden_dim = 64\n",
    "basis_function_dim = 32\n",
    "output_dim = 1  # For example, z as output\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 5\n",
    "l1_lambda = 0.5\n",
    "l2_lambda = 0.05\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasisFunctionLearner(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(BasisFunctionLearner, self).__init__()\n",
    "        # Define a fully connected network with ReLU activations\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc6 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.relu(self.fc5(x))\n",
    "        return self.fc6(x)  # This represents the basis functions, Φ(x)\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.query = nn.Linear(input_dim, hidden_dim)\n",
    "        self.key = nn.Linear(input_dim, hidden_dim)\n",
    "        self.value = nn.Linear(input_dim, hidden_dim)\n",
    "        self.ff_layer_0 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        self.ff_layer_1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "\n",
    "        dotp = torch.matmul(query, key.transpose(-2, -1)) / (query.size(-1) ** 0.5)\n",
    "        attention_weights = F.softmax(dotp, dim=-1)\n",
    "        weighted_sum = torch.matmul(attention_weights, value)\n",
    "        x = weighted_sum + query  # Adding the query for residual connection\n",
    "        x = F.layer_norm(x, x.size()[1:])\n",
    "\n",
    "        dense_out_0 = F.relu(self.ff_layer_0(x))\n",
    "        x = x + self.ff_layer_1(dense_out_0)\n",
    "        x = F.layer_norm(x, x.size()[1:])\n",
    "\n",
    "        return x\n",
    "\n",
    "class WeightsGenerator(nn.Module):\n",
    "    def __init__(self, basis_function_dim, hidden_dim=512, attention_layers=8):\n",
    "        super(WeightsGenerator, self).__init__()\n",
    "        self.top_attention_layer = AttentionLayer(basis_function_dim, hidden_dim)\n",
    "        self.attention_layers = nn.ModuleList([AttentionLayer(hidden_dim, hidden_dim) for _ in range(attention_layers)])\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.final_dense = nn.Linear(hidden_dim, basis_function_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.top_attention_layer(x)\n",
    "        for attention_layer in self.attention_layers:\n",
    "            x = attention_layer(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        final_weights = self.final_dense(x)\n",
    "        return final_weights\n",
    "\n",
    "class FewShotRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, basis_function_dim, output_dim):\n",
    "        super(FewShotRegressionModel, self).__init__()\n",
    "        self.basis_learner = BasisFunctionLearner(input_dim, hidden_dim, basis_function_dim)\n",
    "        self.weights_generator = WeightsGenerator(basis_function_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Step 1: Generate basis functions Φ(x)\n",
    "        basis_functions = self.basis_learner(x)\n",
    "\n",
    "        # Step 2: Generate weights w\n",
    "        weights = self.weights_generator(basis_functions)\n",
    "\n",
    "        # Step 3: Compute final prediction z = Φ(x) * w\n",
    "        return weights, torch.diag(torch.matmul(basis_functions, weights.T))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 300, 2) (60000, 300, 1) (10000, 784, 2) (10000, 784, 1)\n",
      "Data loaded: train_x (60000, 50, 2) test_x (60000, 50, 2) train_y (60000, 50, 1) test_y (60000, 50, 1)\n",
      "(60000, 300, 2) (60000, 300, 1) (10000, 784, 2) (10000, 784, 1)\n",
      "Data loaded: train_x (10000, 50, 2) test_x (10000, 734, 2) train_y (10000, 50, 1) test_y (10000, 734, 1)\n"
     ]
    }
   ],
   "source": [
    "class MnistDataset(Dataset):\n",
    "    def __init__(self, num_shot, num_val, train=True, bmaml=False):\n",
    "        self.num_shot = num_shot\n",
    "        self.bmaml = bmaml\n",
    "        self.dim_input = 2\n",
    "        self.dim_output = 1\n",
    "\n",
    "        # Load data from the pickle file\n",
    "        with open('mnist_data/mnist.pkl', 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            print(data['meta_train_x'].shape, data['meta_train_y'].shape, data['meta_test_x'].shape, data['meta_test_y'].shape)\n",
    "\n",
    "        # Select train or test data\n",
    "        if train:\n",
    "            data_str = 'train'\n",
    "        else:\n",
    "            data_str = 'test'\n",
    "            \n",
    "        # Create dataset with train_x, train_y, test_x, test_y\n",
    "        self.train_x = data['meta_' + data_str + '_x'][:, :num_shot, :]\n",
    "        self.train_y = data['meta_' + data_str + '_y'][:, :num_shot, :]\n",
    "        self.test_x = data['meta_' + data_str + '_x'][:, num_shot:num_shot+num_val, :]\n",
    "        self.test_y = data['meta_' + data_str + '_y'][:, num_shot:num_shot+num_val, :]\n",
    "\n",
    "        print('Data loaded: train_x', self.train_x.shape, 'test_x', self.test_x.shape, \n",
    "              'train_y', self.train_y.shape, 'test_y', self.test_y.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of examples\n",
    "        return self.train_x.shape[0]\n",
    "\n",
    "    def __getitem__(self, indx):\n",
    "        # Fetch context and target data based on index\n",
    "        context_x = torch.tensor(self.train_x[indx], dtype=torch.float32)\n",
    "        context_y = torch.tensor(self.train_y[indx], dtype=torch.float32)\n",
    "        target_x = torch.tensor(self.test_x[indx], dtype=torch.float32)\n",
    "        target_y = torch.tensor(self.test_y[indx], dtype=torch.float32)\n",
    "\n",
    "        if self.bmaml:\n",
    "            # If bmaml is True, concatenate context and target data for leader_x and leader_y\n",
    "            leader_x = torch.cat([context_x, target_x], dim=0)\n",
    "            leader_y = torch.cat([context_y, target_y], dim=0)\n",
    "            return context_x, context_y, leader_x, leader_y, target_x, target_y\n",
    "\n",
    "        return context_x, context_y, target_x, target_y\n",
    "\n",
    "def create_dataloader(num_shot, num_val, batch_size, train=True, bmaml=False):\n",
    "    dataset = MnistDataset(num_shot=num_shot, num_val=num_val, train=train, bmaml=bmaml)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def custom_loss_function(predictions, targets, weights, l1_lambda=0.001, l2_lambda=0.0001):\n",
    "    mse_loss = nn.MSELoss()(predictions, targets)\n",
    "    l1_loss = l1_lambda * torch.norm(weights, p=1)\n",
    "    l2_loss = l2_lambda * torch.norm(weights, p=2)\n",
    "    return mse_loss + l1_loss + l2_loss\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = create_dataloader(num_shot, num_val, batch_size, train=True)\n",
    "test_loader = create_dataloader(num_shot, 784-num_val, batch_size, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context x, context y, target x, target y: (torch.Size([128, 50, 2]), torch.Size([128, 50, 1]), torch.Size([128, 50, 2]), torch.Size([128, 50, 1]))\n",
      "Context x, context y, target x, target y: (torch.Size([128, 50, 2]), torch.Size([128, 50, 1]), torch.Size([128, 734, 2]), torch.Size([128, 734, 1]))\n"
     ]
    }
   ],
   "source": [
    "for t in train_loader:\n",
    "  print(f\"Context x, context y, target x, target y: {t[0].shape, t[1].shape, t[2].shape, t[3].shape}\")\n",
    "  break\n",
    "\n",
    "for t in test_loader:\n",
    "  print(f\"Context x, context y, target x, target y: {t[0].shape, t[1].shape, t[2].shape, t[3].shape}\")\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FewShotRegressionModel(input_dim, hidden_dim, basis_function_dim, output_dim).to(device)\n",
    "maml = l2l.algorithms.MAML(model, lr=learning_rate, first_order=False, allow_unused=True)\n",
    "opt = optim.Adam(maml.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (context_x, context_y, target_x, target_y) in enumerate(train_loader):\n",
    "    meta_train_loss = 0.0\n",
    "    context_x, context_y, target_x, target_y = context_x.to(device), context_y.to(device), target_x.to(device), target_y.to(device)\n",
    "    effective_batch_size = context_x.size(0)\n",
    "    for i in range(effective_batch_size):\n",
    "        learner = maml.clone(first_order=True)\n",
    "        x_support, y_support = context_x[i], context_y[i]\n",
    "        x_query, y_query = target_x[i], target_y[i]\n",
    "        y_support = y_support.view(-1)\n",
    "        y_query = y_query.view(-1)\n",
    "        for _ in range(num_epochs):\n",
    "            wts, predictions = learner(x_support)\n",
    "            loss = custom_loss_function(predictions, y_support, wts)\n",
    "            learner.adapt(loss)\n",
    "        wts, predictions = learner(x_query)\n",
    "        loss = custom_loss_function(predictions, y_query, wts)\n",
    "        meta_train_loss += loss\n",
    "\n",
    "    meta_train_loss /= effective_batch_size\n",
    "    if idx % 10 == 0:\n",
    "        print(f\"Iteration: {idx+1}, Meta train loss: {meta_train_loss}\")\n",
    "\n",
    "    if idx % 50 == 0:\n",
    "        torch.save(model.state_dict(), 'mnist_model_weights.pth')\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    meta_train_loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights\n",
    "torch.save(model.state_dict(), 'mnist_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/DATA/sujit_2021cs35/anaconda3/envs/cv/lib/python3.9/site-packages/learn2learn/algorithms/maml.py\", line 159, in adapt\n",
      "    gradients = grad(loss,\n",
      "  File \"/DATA/sujit_2021cs35/anaconda3/envs/cv/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 303, in grad\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 2; 10.76 GiB total capacity; 9.84 GiB already allocated; 3.44 MiB free; 9.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn2learn: Maybe try with allow_nograd=True and/or allow_unused=True ?\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'gradients' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m wts, predictions \u001b[38;5;241m=\u001b[39m learner(x_support)\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m custom_loss_function(predictions, y_support, wts)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m wts, predictions \u001b[38;5;241m=\u001b[39m learner(x_query)\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m custom_loss_function(predictions, y_query, wts)\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.9/site-packages/learn2learn/algorithms/maml.py:169\u001b[0m, in \u001b[0;36mMAML.adapt\u001b[0;34m(self, loss, first_order, allow_unused, allow_nograd)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearn2learn: Maybe try with allow_nograd=True and/or allow_unused=True ?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Update the module\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule \u001b[38;5;241m=\u001b[39m maml_update(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr, \u001b[43mgradients\u001b[49m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'gradients' referenced before assignment"
     ]
    }
   ],
   "source": [
    "#Load model weights\n",
    "model.load_state_dict(torch.load('mnist_model_weights_450.pth'))\n",
    "\n",
    "# run the test data\n",
    "model.eval()\n",
    "meta_test_loss = 0.0\n",
    "for idx, (context_x, context_y, target_x, target_y) in enumerate(test_loader):\n",
    "    context_x, context_y, target_x, target_y = context_x.to(device), context_y.to(device), target_x.to(device), target_y.to(device)\n",
    "    effective_batch_size = context_x.size(0)\n",
    "    for i in range(effective_batch_size):\n",
    "        learner = maml.clone(first_order=True)\n",
    "        x_support, y_support = context_x[i], context_y[i]\n",
    "        x_query, y_query = target_x[i], target_y[i]\n",
    "        y_support = y_support.view(-1)\n",
    "        y_query = y_query.view(-1)\n",
    "        wts, predictions = learner(x_support)\n",
    "        loss = custom_loss_function(predictions, y_support, wts)\n",
    "        learner.adapt(loss)\n",
    "        wts, predictions = learner(x_query)\n",
    "        loss = custom_loss_function(predictions, y_query, wts)\n",
    "        meta_test_loss += loss\n",
    "    \n",
    "    meta_test_loss /= effective_batch_size\n",
    "    if idx % 10 == 0:\n",
    "        print(f\"Iteration: {idx+1}, Meta test loss: {meta_test_loss}\")\n",
    "    \n",
    "print(f\"Final Meta test loss: {meta_test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
